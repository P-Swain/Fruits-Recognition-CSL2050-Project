{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9036a90",
   "metadata": {},
   "source": [
    "**Cloud-Optimized Image Feature Extraction with PCA:** \n",
    "\n",
    "I used Google Cloud to process and analyze large-scale image data while managing memory efficiently. Here’s how I approached the task:\n",
    "\n",
    "* Feature Extraction:\n",
    "\n",
    "Extracted features like Pixel Intensity, Color Histogram, HOG, and CNN Intermediate Layer from resized images (50x50).\n",
    "\n",
    "* Incremental PCA:\n",
    "\n",
    "Applied Incremental PCA to reduce dimensions in batches, ensuring memory efficiency. Tested different numbers of components to find the best configuration.\n",
    "\n",
    "* Google Cloud Usage:\n",
    "\n",
    "Leveraged cloud computing resources for processing and cloud storage for managing large datasets and intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb20c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cloud-Optimized Image Feature Extraction with PCA ---\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend for cloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Memory management configuration\n",
    "MAX_MEMORY_USAGE = 0.8  # Target max memory usage (80%)\n",
    "BATCH_SIZE = 500  # Reduced from 1000 for cloud environments\n",
    "RESIZE_DIM = (50, 50)  # Reduced image size\n",
    "HIST_BINS = 32  # Reduced from 256 for color histograms\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage percentage\"\"\"\n",
    "    import psutil\n",
    "    return psutil.virtual_memory().percent / 100\n",
    "\n",
    "def safe_image_read(file_path):\n",
    "    \"\"\"Robust image reading with error handling\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(file_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not read image: {file_path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        return cv2.resize(img, RESIZE_DIM)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_color_histogram(image, bins=HIST_BINS):\n",
    "    \"\"\"Memory-efficient color histogram\"\"\"\n",
    "    # Process each channel separately to reduce peak memory\n",
    "    features = []\n",
    "    for i in range(3):\n",
    "        hist = cv2.calcHist([image], [i], None, [bins], [0, 256])\n",
    "        hist = cv2.normalize(hist, hist).flatten()\n",
    "        features.append(hist)\n",
    "        if get_memory_usage() > MAX_MEMORY_USAGE:\n",
    "            gc.collect()\n",
    "    return np.concatenate(features)\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    \"\"\"HOG feature extraction with memory check\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    return hog(gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
    "              feature_vector=True)\n",
    "\n",
    "class CNNFE:\n",
    "    \"\"\"Lazy-loaded CNN feature extractor to reduce memory footprint\"\"\"\n",
    "    _model = None\n",
    "    \n",
    "    @classmethod\n",
    "    def extract(cls, image):\n",
    "        if cls._model is None:\n",
    "            from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "            from tensorflow.keras.models import Model\n",
    "            base_model = VGG16(weights='imagenet', include_top=False, \n",
    "                             input_shape=(*RESIZE_DIM, 3))\n",
    "            cls._model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "        \n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        return cls._model.predict(preprocess_input(image)).flatten()\n",
    "\n",
    "def process_batch(batch_files, folder_path, feature_type):\n",
    "    \"\"\"Process a batch of files with memory management\"\"\"\n",
    "    batch_features = []\n",
    "    batch_labels = []\n",
    "    \n",
    "    for file in batch_files:\n",
    "        file_path = join(folder_path, file)\n",
    "        if not isfile(file_path):\n",
    "            continue\n",
    "            \n",
    "        img = safe_image_read(file_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "            \n",
    "        # Feature extraction\n",
    "        if feature_type == 'pixel_intensity':\n",
    "            features = img.flatten() / 255.0\n",
    "        elif feature_type == 'color_histogram':\n",
    "            features = extract_color_histogram(img)\n",
    "        elif feature_type == 'hog':\n",
    "            features = extract_hog_features(img)\n",
    "        elif feature_type == 'cnn':\n",
    "            features = CNNFE.extract(img)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid feature type: {feature_type}\")\n",
    "            \n",
    "        batch_features.append(features)\n",
    "        \n",
    "        # Label extraction\n",
    "        match = re.search(r'([A-Za-z\\s]+)(?=\\s\\d+$)', os.path.basename(folder_path))\n",
    "        if match:\n",
    "            batch_labels.append(match.group(1))\n",
    "            \n",
    "        # Memory management\n",
    "        if get_memory_usage() > MAX_MEMORY_USAGE:\n",
    "            gc.collect()\n",
    "            \n",
    "    return np.array(batch_features, dtype=np.float32), np.array(batch_labels)\n",
    "\n",
    "def import_images_in_batches(path, feature_type):\n",
    "    \"\"\"Memory-efficient batch processing with caching\"\"\"\n",
    "    cache_file = join(\"cache\", f\"features_{feature_type}.pkl\")\n",
    "    \n",
    "    # Try loading from cache\n",
    "    if isfile(cache_file):\n",
    "        try:\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        except:\n",
    "            os.remove(cache_file)\n",
    "    \n",
    "    # Process folders in sorted order for consistency\n",
    "    folders = sorted([f for f in os.listdir(path) if os.path.isdir(join(path, f))])\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for folder in tqdm(folders, desc=\"Processing folders\"):\n",
    "        folder_path = join(path, folder)\n",
    "        files = [f for f in os.listdir(folder_path) if isfile(join(folder_path, f))]\n",
    "        \n",
    "        # Process in smaller batches for memory efficiency\n",
    "        for i in range(0, len(files), BATCH_SIZE // 5):\n",
    "            batch_files = files[i:i + BATCH_SIZE // 5]\n",
    "            features, labels = process_batch(batch_files, folder_path, feature_type)\n",
    "            \n",
    "            if len(features) > 0:\n",
    "                all_features.append(features)\n",
    "                all_labels.append(labels)\n",
    "                \n",
    "            if get_memory_usage() > MAX_MEMORY_USAGE:\n",
    "                gc.collect()\n",
    "    \n",
    "    # Combine and save to cache\n",
    "    all_features = np.vstack(all_features)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    os.makedirs(\"cache\", exist_ok=True)\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump((all_features, all_labels), f, protocol=4)  # Protocol 4 for cloud compatibility\n",
    "        \n",
    "    return all_features, all_labels\n",
    "\n",
    "def apply_incremental_pca(features, labels, n_components=2):\n",
    "    \"\"\"Memory-efficient PCA with caching\"\"\"\n",
    "    cache_file = join(\"cache\", f\"pca_{n_components}.pkl\")\n",
    "    \n",
    "    if isfile(cache_file):\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    ipca = IncrementalPCA(n_components=n_components, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Fit in batches\n",
    "    for i in tqdm(range(0, len(features), BATCH_SIZE), desc=\"Fitting PCA\"):\n",
    "        batch = features[i:i + BATCH_SIZE]\n",
    "        ipca.partial_fit(batch)\n",
    "        if get_memory_usage() > MAX_MEMORY_USAGE:\n",
    "            gc.collect()\n",
    "    \n",
    "    # Transform in batches\n",
    "    reduced = []\n",
    "    for i in tqdm(range(0, len(features), BATCH_SIZE), desc=\"Transforming\"):\n",
    "        batch = features[i:i + BATCH_SIZE]\n",
    "        reduced.append(ipca.transform(batch))\n",
    "        if get_memory_usage() > MAX_MEMORY_USAGE:\n",
    "            gc.collect()\n",
    "    \n",
    "    reduced = np.vstack(reduced)\n",
    "    \n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump((reduced, labels), f, protocol=4)\n",
    "        \n",
    "    return reduced, labels\n",
    "\n",
    "def visualize_pca(reduced, labels, feature_type):\n",
    "    \"\"\"Save plot instead of showing for cloud environments\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    unique_labels = np.unique(labels)\n",
    "    cmap = plt.cm.get_cmap('rainbow', len(unique_labels))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        plt.scatter(reduced[mask, 0], reduced[mask, 1], \n",
    "                   label=label, s=20, alpha=0.6, \n",
    "                   color=cmap(i / len(unique_labels)))\n",
    "    \n",
    "    plt.title(f'PCA - {feature_type}')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    plt.savefig(f\"plots/pca_{feature_type}.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved plot to plots/pca_{feature_type}.png\")\n",
    "\n",
    "def main():\n",
    "    path = 'fruits-360/Training'\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset path not found: {path}\")\n",
    "    \n",
    "    feature_types = ['pixel_intensity', 'color_histogram', 'hog', 'cnn']\n",
    "    for i in range(2, 111):\n",
    "        for feature_type in feature_types:\n",
    "            print(f\"\\nProcessing {feature_type} features...\")\n",
    "            \n",
    "            # Load or process features\n",
    "            features, labels = import_images_in_batches(path, feature_type)\n",
    "            \n",
    "            # Apply PCA\n",
    "            reduced, labels = apply_incremental_pca(features, labels,n_components=i)\n",
    "            \n",
    "            # Visualize and save results\n",
    "            visualize_pca(reduced, labels, feature_type)\n",
    "            \n",
    "            # Explicit cleanup\n",
    "            del features, reduced\n",
    "            gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure for cloud environments\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduce TensorFlow logging\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise\n",
    "    # this code will not work here as i have used it on cloud and obtained the results and used kmeans to create 440 models \n",
    "    # and saved them as model folder which i used to find best p_components by calculating silhouette score and storing in \n",
    "    # /home/lalit-mohan/model testing/clustering_results_20250411_223025.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84561b5",
   "metadata": {},
   "source": [
    "**Results for KMeans Clustering Evaluation:**\n",
    "\n",
    "* Feature Extraction:\n",
    "\n",
    "I extracted four feature types: Color Histogram, Pixel Intensity, HOG, and CNN Intermediate Layer.\n",
    "\n",
    "* PCA for Dimensionality Reduction:\n",
    "\n",
    "I applied PCA to reduce the feature dimensions and tested various numbers of components.\n",
    "\n",
    "* KMeans Clustering:\n",
    "\n",
    "I used KMeans to cluster the data for different values of k and calculated the Silhouette Score.\n",
    "\n",
    "* Optimal k Selection:\n",
    "\n",
    "I identified the best k for each feature by selecting the highest silhouette score.\n",
    "\n",
    "* Results:\n",
    "\n",
    "The optimal number of PCA components was 2 for all feature types.\n",
    "\n",
    "* Conclusion:\n",
    "\n",
    "The best-performing model was based on [best feature type], with k = [best k] and the highest silhouette score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f34f87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model      | Best k | Max Score  | Avg Score \n",
      "--------------------------------------------------\n",
      "cnn        | 2      | 0.3327     | 0.1855    \n",
      "pixel_intensity | 2      | 0.3572     | 0.1768    \n",
      "hog        | 2      | 0.3310     | 0.1175    \n",
      "color_histogram | 2      | 0.3592     | 0.3029    \n",
      "\n",
      " Overall Best Model:\n",
      "Model: color_histogram\n",
      "Best k: 2\n",
      "Silhouette Score: 0.3592\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pickle file\n",
    "with open('/home/lalit-mohan/model testing/clustering_results_20250411_223025.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Define colors for models\n",
    "colors = ['blue', 'green', 'orange', 'purple', 'brown', 'teal', 'gray']\n",
    "markers = ['o', 's', 'D', '^', 'v', 'p', '*']\n",
    "\n",
    "# Loop through each model and plot\n",
    "for i, (model, model_results) in enumerate(results.items()):\n",
    "    k_values = sorted(model_results.keys())\n",
    "    silhouette_scores = [model_results[k]['silhouette'] for k in k_values]\n",
    "    \n",
    "    best_k = k_values[silhouette_scores.index(max(silhouette_scores))]\n",
    "    best_score = max(silhouette_scores)\n",
    "\n",
    "    # Plot line\n",
    "    plt.plot(k_values, silhouette_scores, marker=markers[i % len(markers)],\n",
    "             color=colors[i % len(colors)], label=f'{model.upper()}')\n",
    "\n",
    "    # Highlight best point\n",
    "    plt.scatter([best_k], [best_score], color=colors[i % len(colors)],\n",
    "                edgecolor='black', s=100, zorder=5)\n",
    "\n",
    "    # Add dashed line for best k\n",
    "    plt.axvline(best_k, color=colors[i % len(colors)], linestyle='--', alpha=0.3)\n",
    "\n",
    "# Plot settings\n",
    "plt.title('Silhouette Score vs Number of Clusters for All Models')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.legend(title='Model')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "summary = {}\n",
    "\n",
    "for model, model_results in results.items():\n",
    "    k_values = sorted(model_results.keys())\n",
    "    silhouette_scores = [model_results[k]['silhouette'] for k in k_values]\n",
    "\n",
    "    max_score = max(silhouette_scores)\n",
    "    best_k = k_values[silhouette_scores.index(max_score)]\n",
    "    avg_score = np.mean(silhouette_scores)\n",
    "\n",
    "    summary[model] = {\n",
    "        'best_k': best_k,\n",
    "        'max_score': max_score,\n",
    "        'avg_score': avg_score\n",
    "    }\n",
    "\n",
    "# Print model-wise summary\n",
    "print(f\"{'Model':<10} | {'Best k':<6} | {'Max Score':<10} | {'Avg Score':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for model, stats in summary.items():\n",
    "    print(f\"{model:<10} | {stats['best_k']:<6} | {stats['max_score']:<10.4f} | {stats['avg_score']:<10.4f}\")\n",
    "\n",
    "# Find overall best model\n",
    "best_model = max(summary.items(), key=lambda x: x[1]['max_score'])\n",
    "print(\"\\n Overall Best Model:\")\n",
    "print(f\"Model: {best_model[0]}\")\n",
    "print(f\"Best k: {best_model[1]['best_k']}\")\n",
    "print(f\"Silhouette Score: {best_model[1]['max_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d87f08",
   "metadata": {},
   "source": [
    "I used the below given code to make kmeans model for all best result giving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Mapping of feature files to corresponding output files\n",
    "feature_files = {\n",
    "    \"data/images_color_histogram_50x50.pkl\": \"robo_mod/color_histogram_kmeans.pkl\",\n",
    "    \"data/images_hog_50x50.pkl\": \"robo_mod/hog_kmeans.pkl\",\n",
    "    \"data/images_pixel_intensity_50x50.pkl\": \"robo_mod/pixel_intensity_kmeans.pkl\"\n",
    "}\n",
    "\n",
    "# Number of clusters (modify as needed)\n",
    "NUM_CLUSTERS = 140\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"robo_mod\", exist_ok=True)\n",
    "\n",
    "for feature_file, kmeans_file in feature_files.items():\n",
    "    print(f\"\\nProcessing {feature_file} → {kmeans_file}\")\n",
    "\n",
    "    # Load the original feature data\n",
    "    with open(feature_file, \"rb\") as f:\n",
    "        features = pickle.load(f)\n",
    "\n",
    "    # Check if data is a tuple and extract the first element (which should be the NumPy array)\n",
    "    if isinstance(features, tuple):\n",
    "        X = features[0]\n",
    "        print(f\"Data extracted from tuple, first element shape: {X.shape}\")\n",
    "    elif isinstance(features, np.ndarray):\n",
    "        X = features\n",
    "    else:\n",
    "        print(f\"{feature_file} does not contain a NumPy array or tuple.\")\n",
    "        continue\n",
    "\n",
    "    # Apply PCA to reduce dimensionality to 2 components\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    # Fit KMeans on the PCA-reduced features\n",
    "    kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\n",
    "    kmeans.fit(X_pca)\n",
    "\n",
    "    # Save the KMeans model\n",
    "    with open(kmeans_file, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "\n",
    "    print(f\"Saved KMeans model at {kmeans_file} (n_clusters = {NUM_CLUSTERS})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3399925",
   "metadata": {},
   "source": [
    "used this code to have some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d04aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.io import imread_collection\n",
    "from skimage.color import rgb2hsv\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Paths and settings\n",
    "model_path = 'robo_mod/color_histogram_kmeans.pkl'\n",
    "dataset_path = \"/home/lalit-mohan/fruits-360/Training\"\n",
    "feature_cache_path = 'robo_mod/color_histogram_features.pkl'\n",
    "bins = 8\n",
    "\n",
    "# Custom function to compute histogram for HSV image\n",
    "def compute_histogram(image, bins=8):\n",
    "    hist = []\n",
    "    for channel in range(image.shape[2]):\n",
    "        channel_hist, _ = np.histogram(image[:, :, channel], bins=bins, range=(0, 1), density=True)\n",
    "        hist.extend(channel_hist)\n",
    "    return np.array(hist)\n",
    "\n",
    "# Extract color histogram from HSV image\n",
    "def extract_color_histogram(image, bins=8):\n",
    "    image = image / 255.0\n",
    "    hsv_image = rgb2hsv(image)\n",
    "    return compute_histogram(hsv_image, bins)\n",
    "\n",
    "# Load KMeans model\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Model file does not exist:\", model_path)\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    model = joblib.load(model_path)\n",
    "    if not isinstance(model, KMeans):\n",
    "        print(\"Loaded model is not a KMeans instance.\")\n",
    "        exit()\n",
    "    print(\"KMeans model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load images from dataset\n",
    "images = imread_collection(f\"{dataset_path}/*/*.jpg\")\n",
    "print(f\"{len(images)} images loaded.\")\n",
    "\n",
    "# Load or compute features\n",
    "if os.path.exists(feature_cache_path):\n",
    "    print(\"Loading features from cache...\")\n",
    "    with open(feature_cache_path, 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "else:\n",
    "    print(\"Extracting features...\")\n",
    "    features = [extract_color_histogram(img, bins) for img in tqdm(images, desc=\"Extracting features\")]\n",
    "    features = np.array(features).astype(np.float64)  # Ensure dtype is float64\n",
    "\n",
    "    print(\"Saving features to cache...\")\n",
    "    with open(feature_cache_path, 'wb') as f:\n",
    "        pickle.dump(features, f)\n",
    "\n",
    "# Apply PCA\n",
    "print(\"Reducing dimensions with PCA...\")\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(features).astype(np.float32)  # Ensure dtype is float64\n",
    "\n",
    "# Predict with KMeans\n",
    "print(\"Predicting cluster labels...\")\n",
    "print(\"Data type of reduced_features before prediction:\", reduced_features.dtype)\n",
    "predictions = model.predict(reduced_features)\n",
    "\n",
    "# Output sample results\n",
    "print(\"Predictions complete. Sample output:\", predictions[:1000])\n",
    "\n",
    "# Get the image paths of the first 10 images\n",
    "image_paths = images.files[:1000]  # This is the list of file paths\n",
    "\n",
    "# Extract image names and parent directories from file paths\n",
    "image_info = [(os.path.basename(img_path), os.path.dirname(img_path)) for img_path in image_paths]\n",
    "\n",
    "# Map image names and parent directories to predicted clusters\n",
    "image_cluster_mapping = {\n",
    "    (image_info[i][0], image_info[i][1]): predictions[i] for i in range(1000)\n",
    "}\n",
    "\n",
    "# Display the mapping of image names and parent directories to clusters\n",
    "print(\"Image Name, Parent Directory to Predicted Cluster Mapping:\")\n",
    "for (image_name, parent_dir), cluster in image_cluster_mapping.items():\n",
    "    print(f\"Image: {image_name} (Parent Directory: {parent_dir}) => Predicted Cluster: {cluster}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
